{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf0M9cOH7OYRaUXAWiidGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MANOJ-S-NEGI/Data_science/blob/main/NLP_ASSIGNMENT_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP_ASSIGNMENT_01\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**1. Explain One-Hot Encoding**\n",
        "\n",
        "sol:\n",
        "\n",
        "One-hot encoding is a technique used in machine learning and natural language processing to represent categorical variables as binary vectors.\n",
        "\n",
        "- In this encoding, each category is represented by a unique binary number, where all values are zero except for the index corresponding to the category, which is set to one.\n",
        "\n",
        "- For example, if you're encoding colors (red, green, blue), red might be represented as [1, 0, 0], green as [0, 1, 0], and blue as [0, 0, 1].\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**2. Explain Bag of Words**\n",
        "\n",
        "sol:\n",
        "\n",
        "- The Bag of Words model is a technique used in natural language processing for text analysis.\n",
        "\n",
        "- It involves representing text data as an unordered set of words, disregarding grammar and word order.\n",
        "\n",
        "- The frequency of each word in a given text is what matters.\n",
        "\n",
        "- This representation is often used as a feature in machine learning models for tasks like sentiment analysis, document classification, and more.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**3. Explain Bag of N-Grams**\n",
        "\n",
        "sol:\n",
        "\n",
        "Similar to Bag of Words, the Bag of N-Grams model represents text data by considering sequences of adjacent words (n-grams) instead of individual words.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**4. Explain TF-IDF**\n",
        "\n",
        "sol:\n",
        "\n",
        "- TF-IDF is a statistical measure used to evaluate the importance of a word within a document relative to a collection of documents (corpus).\n",
        "\n",
        "- It combines the term frequency (TF), which measures how frequently a term appears in a document, with the inverse document frequency (IDF), which measures how rare a term is across documents in a corpus.\n",
        "\n",
        "- TF-IDF is used to weigh the importance of each word in a document and is often used for information retrieval and text mining tasks.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**5. What is OOV problem?**\n",
        "\n",
        "sol:\n",
        "\n",
        "The OOV problem arises in natural language processing when a word or phrase that is encountered in test or application data has not been seen during the training of a language model. This can cause issues because the model doesn't have any information about how to handle this unseen word.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**6. What are word embeddings?**\n",
        "\n",
        "sol:\n",
        "\n",
        "- Word embeddings are vector representations of words in a continuous vector space.\n",
        "- They capture semantic relationships between words based on their usage in context.\n",
        "- In word embeddings, similar words are located closer to each other in the vector space.\n",
        "- Word embeddings are learned from large amounts of text data using techniques like Word2Vec, GloVe, and more.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**7. Explain Continuous bag of words (CBOW)**\n",
        "\n",
        "sol:\n",
        "\n",
        "- CBOW is a word embedding technique where the goal is to predict a target word based on its context (surrounding words).\n",
        "\n",
        "- It does this by taking the average of the word vectors in the context to predict the target word.\n",
        "\n",
        "- CBOW is trained by minimizing the difference between the predicted and actual target words over a large dataset.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**8. Explain SkipGram**\n",
        "\n",
        "sol:\n",
        "\n",
        "- SkipGram is another word embedding technique, but it works in the opposite way of CBOW. Instead of predicting a target word from its context, SkipGram aims to predict the context words given a target word.\n",
        "\n",
        "- This approach is often preferred for capturing more fine-grained relationships between words\n",
        "\n",
        "---\n",
        "---\n",
        "<br>\n",
        "\n",
        "**9. Explain Glove Embeddings.**\n",
        "\n",
        "sol:\n",
        "\n",
        "- GloVe (Global Vectors) is a word embedding technique that combines the strengths of both global (corpus-wide) and local (context-based) word embeddings.\n",
        "- It's trained by factorizing a global word co-occurrence matrix, which captures the probability of word pairs occurring together. GloVe embeddings are known for capturing semantic relationships effectively.\n",
        "\n",
        "---\n",
        "---\n",
        "<br>"
      ],
      "metadata": {
        "id": "2hrr4WCQg-MY"
      }
    }
  ]
}