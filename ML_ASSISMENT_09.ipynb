{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF0RXN8711yVIaxWn/SosE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MANOJ-S-NEGI/Data_science/blob/main/ML_ASSISMENT_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML_ASSISMENT_09"
      ],
      "metadata": {
        "id": "LLePg87VIvC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "Feature engineering is the process of selecting, transforming, and creating relevant features from raw data to improve the performance of machine learning models. It involves crafting the input variables (features) in a way that the model can learn more effectively and make better predictions. Feature engineering is crucial because the quality and relevance of features directly impact the model's ability to capture underlying patterns in the data.\n",
        "\n",
        "**Aspects of Feature Engineering:**\n",
        "\n",
        "- **Feature Selection:** Choosing the most relevant features and discarding irrelevant or redundant ones.\n",
        "- **Feature Transformation:** Modifying features to make them more suitable for the model (e.g., scaling, normalization, log-transform).\n",
        "- **Feature Creation:** Generating new features by combining or transforming existing ones (e.g., polynomial features, interaction terms).\n",
        "- **Handling Missing Values:** Deciding how to handle missing or null values in features.\n",
        "- **Encoding Categorical Variables:** Converting categorical variables into numerical representations (e.g., one-hot encoding, label encoding).\n",
        "- **Text and NLP Feature Engineering:** For text data, this involves tokenization, stemming, removing stop words, and creating numerical representations like TF-IDF or word embeddings.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "g9A-YpaqIxYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "Feature selection is the process of choosing a subset of the most relevant features from the original set of features. The aim is to reduce dimensionality, enhance model interpretability, and potentially improve model performance by eliminating noise and reducing overfitting.\n",
        "\n",
        "**Methods of Feature Selection:**\n",
        "\n",
        "- **Filter Methods:** These methods evaluate the relevance of features based on statistical measures or other domain-specific criteria before the model is trained. Examples include correlation, chi-square, and mutual information.\n",
        "\n",
        "- **Wrapper Methods:** These methods involve training the model iteratively on different subsets of features and measuring their performance. Examples include forward selection, backward elimination, and recursive feature elimination.\n",
        "\n",
        "- **Embedded Methods:** These methods combine feature selection with the model training process. Regularization techniques like Lasso (L1 regularization) encourage sparsity in feature weights, automatically selecting important features.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "dKT3G4mqIxcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**1. Filter Approach:**\n",
        "\n",
        "In the filter approach, features are selected based on their individual relevance to the target variable. Various statistical or domain-specific measures are used to evaluate the importance of each feature independently of the machine learning model.\n",
        "\n",
        "  - **Pros:** Faster and less computationally intensive as it doesn't involve training the model repeatedly. It can handle a large number of features.\n",
        "  - **Cons:** May not consider feature interactions specific to the model. Ignore the model's actual performance on the dataset.\n",
        "\n",
        "\n",
        "**2. Wrapper Approach:**\n",
        "\n",
        "The wrapper approach involves training the machine learning model iteratively on different subsets of features and evaluating the model's performance on a validation set. The goal is to find the subset of features that leads to the best model performance.\n",
        "\n",
        "  - **Pros:** Considers feature interactions more effectively as the model's performance is directly involved. Can lead to better model performance.\n",
        "  - **Cons:** Computationally expensive and can lead to overfitting if not used carefully. Requires retraining the model multiple times.\n",
        "\n",
        "  <br><br>\n"
      ],
      "metadata": {
        "id": "WXZ5IbwcIxgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **4.**\n",
        "**i. Describe the overall feature selection process.**\n",
        "\n",
        "**ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**1. Feature Selection Process:**\n",
        "- Data Collection: Gather the raw data.\n",
        "- Data Preprocessing: Clean, transform, and prepare the data for analysis.\n",
        "- Feature Engineering: Select, transform, or create features.\n",
        "- Feature Selection: Choose the most relevant features using filter, wrapper, or embedded methods.\n",
        "- Model Training: Train the machine learning model using the selected features.\n",
        "- Model Evaluation: Evaluate the model's performance on a validation set or through cross-validation.\n",
        "- Iterative Refinement: If necessary, go back and refine the feature engineering and selection steps based on model performance.\n",
        "\n",
        "**2. Feature Extraction Principle:**\n",
        "- Feature extraction involves transforming raw data into a lower-dimensional space while preserving important information. An example is Principal Component Analysis (PCA) for numerical data.\n",
        "- PCA identifies the directions (principal components) of maximum variance in the data and projects the data onto these components.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "ycGBBmMxIxj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **5. Describe the feature engineering process in the sense of a text categorization issue.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "Feature Engineering for Text Categorization, the process involves:\n",
        "\n",
        "- **Text Tokenization:** Splitting text into words or tokens.\n",
        "- **Removing Stop Words:** Eliminating common words that don't carry much meaning.\n",
        "- **Stemming or Lemmatization:** Reducing words to their base or root form.\n",
        "- **TF-IDF Calculation:** Computing Term Frequency-Inverse Document Frequency to weigh word importance.\n",
        "- **Word Embeddings:** Converting words into dense vector representations using techniques like Word2Vec or GloVe.\n",
        "- **Feature Selection:** Selecting the most relevant words or n-grams using methods like mutual information or chi-square.\n",
        "\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "VH-2P-LUIxoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **6. What makes cosine similarity a good metric for text categorization?**\n",
        " ```\n",
        " A document-term matrix has two rows with values of\n",
        "(2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
        "  \n",
        "```\n",
        "**Find the resemblance in cosine.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "Cosine Similarity for Text Categorization:\n",
        "\n",
        "- Cosine similarity is a metric used to measure the similarity between two vectors.\n",
        "\n",
        "- In text categorization, each document is represented as a vector, where each dimension corresponds to a word or term, and the value represents the frequency or TF-IDF weight of that term in the document.\n",
        "```\n",
        "Given the document-term matrices:\n",
        "Document A: (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
        "Document B: (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
        "Cosine similarity = [Σ(Ai * Bi)] / {sqrt[Σ(Ai^2)] * sqrt[Σ(Bi^2)]}\n",
        "```\n",
        "Calculating the values:\n",
        "```\n",
        "Numerator:\n",
        "(2 * 2) + (3 * 1) + (2 * 0) + (0 * 0) + (2 * 3) + (3 * 2) + (3 * 1) + (0 * 3) + (1 * 1)\n",
        "= 26\n",
        "```\n",
        "```\n",
        "Denominator for Document 1:\n",
        "sqrt((2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2))\n",
        "= sqrt(42)\n",
        "```\n",
        "\n",
        "```\n",
        "Denominator for Document 2:\n",
        "\n",
        "sqrt((2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2))\n",
        "\n",
        "= sqrt(22)\n",
        "```\n",
        "```\n",
        "Cosine similarity:\n",
        "= 26 / (sqrt(42) * sqrt(22))\n",
        "≈ 0.74\n",
        "```\n",
        "The cosine similarity of these two documents is approximately 0.74, indicating a relatively high degree of similarity between them in the vector space.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "ddUO3J9mLeYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **7 i. What is the formula for calculating Hamming distance? Between (10001011) and (11001111), calculate the Hamming gap.**\n",
        "\n",
        "sol:\n",
        "\n",
        "**Counting the Differences:**\n",
        "- There are 2 positions (2nd and 6th) where the bits are different.\n",
        "\n",
        "- The correct Hamming distance between the binary strings 10001011 and 11001111 is 2.\n",
        "\n",
        "\n",
        "**ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).**\n",
        "```\n",
        "Feature Vector A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
        "Feature Vector B: (1, 1, 0, 0, 0, 1, 1, 1)\n",
        "Feature Vector C: (1, 0, 0, 1, 1, 0, 0, 1)\n",
        "```\n",
        "\n",
        "sol:\n",
        "\n",
        "**Jaccard Index (J):**\n",
        "\n",
        "- The Jaccard index is defined as the size of the intersection of two sets divided by the size of their union. In the case of binary vectors, it's the number of matching positions (1s) divided by the total number of positions.\n",
        "```\n",
        "J(A, B) = (Number of common 1s) / (Total number of positions)\n",
        "J(A, B) = 4 / 8 = 0.5\n",
        "J(A, C) = 3 / 8 = 0.375\n",
        "```\n",
        "\n",
        "**Similarity Matching Coefficient (SMC):**\n",
        "\n",
        "- The SMC measures the proportion of matching positions in two binary vectors, ignoring the non-matching positions.\n",
        "\n",
        "```\n",
        "SMC(A, B) = (Number of common 1s) / (Number of 1s in A + Number of 1s in B - Number of common 1s)\n",
        "\n",
        "SMC(A, B) = 4 / (5 + 5 - 4) = 0.66\n",
        "\n",
        "SMC(A, C) = 3 / (5 + 4 - 3) = 0.5\n",
        "```\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "tVrYi7ARLekX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**1. high-dimensional data set**\n",
        "- A high-dimensional data set refers to a collection of data points where each data point is described by a large number of features or attributes.\n",
        "\n",
        "- In other words, the data has a large number of dimensions.\n",
        "\n",
        "- This can make visualization and analysis more challenging, as human perception is limited to three dimensions.\n",
        "\n",
        "- High-dimensional data sets are common in fields such as genomics, image processing, social network analysis, and sensor data, where each data point can be characterized by numerous measurements or descriptors.\n",
        "\n",
        "**2. Difficulties and Solutions**\n",
        "- Curse of Dimensionality: As the number of dimensions increases, the amount of data required to maintain meaningful distances between data points increases exponentially, potentially leading to sparse data.\n",
        "\n",
        "- Computational Complexity: Many machine learning algorithms become computationally intensive as the number of dimensions increases.\n",
        "\n",
        "- Overfitting: High-dimensional data sets are prone to overfitting, where models perform well on training data but generalize poorly to new data.\n",
        "\n",
        "- Visualization: Visualizing data in high dimensions is difficult, as human perception is limited to three dimensions.\n",
        "\n",
        "**Solution:**\n",
        "- Regularization techniques and feature selection methods can help mitigate overfitting by reducing the complexity of the model.\n",
        "\n",
        "- Solution: Dimensionality reduction techniques like Principal Component Analysis (PCA) can help by projecting the data into a lower-dimensional space while preserving important information.\n",
        "\n",
        "- Efficient algorithms and techniques tailored for high-dimensional data, like sparse matrix operations, can be used to handle the computational complexity.\n",
        "\n",
        "- Techniques like t-SNE (t-distributed Stochastic Neighbor Embedding) can be employed to visualize high-dimensional data in lower-dimensional spaces.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "A4lOCWYSLeqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **9. Make a few quick notes on:**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**PCA is an acronym for Personal Computer Analysis**\n",
        "- **PCA (Principal Component Analysis):**\n",
        " - A dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much variance as possible. It's not related to Personal Computer Analysis.\n",
        "\n",
        "**Use of vectors**\n",
        "- Use of Vectors: Vectors are mathematical constructs used to represent quantities that have both magnitude and direction. They are essential in machine learning for representing data points, features, and model parameters.\n",
        "\n",
        "**Embedded technique**\n",
        "- **Embedded Technique:**\n",
        "  - An embedded technique in machine learning refers to methods that automatically learn feature representations as part of the model training process. This can help in capturing relevant information and reducing the dimensionality of the data for better model performance.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "bxvJImZZIjMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **10. Make a comparison between:**\n",
        "\n",
        "**1. Sequential backward exclusion vs. sequential forward selection**\n",
        "sol:\n",
        "\n",
        "**Sequential Backward Exclusion:**\n",
        "\n",
        "- This is a feature selection technique where we start with all features and iteratively remove one feature at a time that has the least impact on the model's performance.\n",
        "- It continues until a stopping criterion is met, such as a certain number of features remaining or a threshold level of performance drop.\n",
        "\n",
        "**Sequential Forward Selection:**\n",
        "- This is a feature selection technique where we start with an empty set of features and iteratively add one feature at a time based on the one that provides the most improvement in model performance.\n",
        "- It continues until a stopping criterion is met.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Function selection methods: filter vs. wrapper**\n",
        "sol:\n",
        "\n",
        "\n",
        "**Filter Methods:**\n",
        "- These are feature selection methods that rely on applying statistical measures to each feature independently to rank them.\n",
        "- Features are selected based on their scores without considering the impact on the specific model to be used.\n",
        "\n",
        "\n",
        "\n",
        "**Wrapper Methods:**\n",
        "- These methods use the model's performance as the evaluation criterion for feature selection.\n",
        "\n",
        "- They involve training and evaluating the model multiple times with different feature subsets.\n",
        "\n",
        "---\n",
        "**3. SMC vs. Jaccard coefficient**\n",
        "\n",
        "sol:\n",
        "\n",
        "**SMC (Simple Matching Coefficient):**\n",
        "\n",
        "- SMC is a similarity coefficient used to compare the similarity between two binary data sets. It calculates the proportion of matching elements between the two sets.\n",
        "\n",
        "- SMC is specifically used for binary data, where each element is either present or absent.\n",
        "\n",
        "- SMC considers only matching elements\n",
        "\n",
        "**Jaccard Coefficient:**\n",
        "\n",
        "- The Jaccard coefficient is a measure of similarity between two sets. It's calculated as the size of the intersection of the sets divided by the size of the union of the sets.\n",
        "\n",
        "- Jaccard coefficient is more general and can be applied to any type of set data.\n",
        "\n",
        "- Jaccard coefficient considers both matching and non-matching elements.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "7W8hcUMlcKZC"
      }
    }
  ]
}