{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcPyO6g45yFe5QpNTmu/+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MANOJ-S-NEGI/Data_science/blob/main/ML_ASSISMENT_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML_ASSISMENT_12"
      ],
      "metadata": {
        "id": "k0nvuwVASLEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **What is prior probability? Give an example.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "It represents the probability assigned to an event based on existing knowledge or historical data, without considering any additional information or observations.\n",
        "\n",
        "**Example of prior probability:**\n",
        "\n",
        "* Imagine we are trying to predict whether it will rain tomorrow in a city and based on historical weather data for that city, we know that it rains on about 30% of days throughout the year. This 30% is our prior probability of rain for any given day, regardless of the current weather conditions.\n",
        "\n",
        "- So, Prior Probability of Rain = 0.30 (or 30%)\n",
        "\n",
        "This is the probability we assign before considering any specific information about the current weather conditions.and later on we update it with new data\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "HTMcPvXASLL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **What is posterior probability? Give an example.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "- posterior probability is the updated probability of an event occurring after incorporating new evidence, information, or observations.\n",
        "\n",
        "- It's calculated using Bayes' theorem, which combines the prior probability with the likelihood of the evidence given the event and a normalization factor.\n",
        "\n",
        "- The posterior probability provides a more informed estimate of the event's likelihood based on both prior beliefs and new data.\n",
        "\n",
        "**Example :**\n",
        "\n",
        "Taking the previous example of rain, now we receive additional information from weather forecasts, which indicates that the barometric pressure is dropping significantly, humidity levels are rising, and winds are shifting.\n",
        "\n",
        "\n",
        "\n",
        "**Using Bayes' theorem**\n",
        "\n",
        "we can update the prior probability to a posterior probability of rain given the new evidence  Now, applying Bayes' theorem:\n",
        "\n",
        "- **Posterior Probability of Rain = (Likelihood of occuring Rain * Prior Probability of Rain) / (Total Likelihood)**\n",
        "\n",
        "   where,\n",
        "  - total likelihood is (Likelihood of Evidence Given Rain * Prior Probability of Rain) + (Likelihood of Evidence Given No Rain * Prior Probability of No Rain)\n",
        "\n",
        "- ```\n",
        "DataPoint:\n",
        "Previous data : rain occurring 30% so non occurring is 70%\n",
        "Updated data  : rain occurring is 3:1\n",
        "Substituting the values:\n",
        "Posterior Probability of Rain\n",
        "= (3 * 0.30) / [(3 * 0.30) + (1 * 0.70)]\n",
        "≈ 0.6\n",
        "```\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "JBi4QYO7SLOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **What is likelihood probability? Give an example.**\n",
        "      \n",
        "      \n",
        "Sol:\n",
        "\n",
        "- Likelihood probability, measures how well a given statistical model explains observed data. It indicates the plausibility of observing the data under a specific model, without incorporating any prior information about the model parameters.\n",
        "\n",
        "- Unlike probabilities, which are typically defined over outcomes or events, likelihood is defined over model parameters and is used to assess how well the model's parameters fit the observed data.\n",
        "\n",
        "**Example of likelihood probability:**\n",
        "```\n",
        "Suppose we are conducting a study on the heights of a certain species of trees in a forest and want to estimate the average height of these trees using a normal distribution model.\n",
        "Now the model assumes that the heights of these trees follow a normal distribution with an unknown mean (μ) and a known standard deviation (σ).\n",
        "\n",
        "Mathematically, the likelihood function L(μ | data) can be expressed as:\n",
        "x =  exp(-(xᵢ - μ)² / (2σ²))\n",
        "so,\n",
        "   (1 / (σ * √(2π)))^x\n",
        "\n",
        "Where:    xᵢ represents each observed data point.\n",
        "          μ is the unknown mean you're trying to estimate.\n",
        "          σ is the known standard deviation.\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "zcl7EgRqupOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **What is the Naïve Bayes classifier? Why is it named so?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "- The Naïve Bayes classifier is a probabilistic machine learning algorithm commonly used for classification tasks.\n",
        "\n",
        "\n",
        "- It's based on Bayes' theorem and assumes that the features (attributes) used to describe data points are conditionally independent of each other given the class label.\n",
        "\n",
        "\n",
        "- This \"naïve\" assumption simplifies the calculations and makes the algorithm computationally efficient, though it might not hold true in all real-world scenarios.\n",
        "\n",
        "\n",
        "- Despite its simplifications and assumptions, Naïve Bayes often performs surprisingly well in various text classification and natural language processing tasks, especially when dealing with high-dimensional data.\n",
        "\n",
        "\n",
        "**The name \"Naïve Bayes\" originates from two sources:**\n",
        "\n",
        "**Naïve Assumption:**\n",
        "\n",
        "- The term \"naïve\" is used because the algorithm makes the assumption that the features are conditionally independent, which might not be true in many cases. In reality, features often have dependencies between them.\n",
        "\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "- The algorithm is based on Bayes' theorem, a fundamental theorem in probability theory that describes how to update the probability of a hypothesis as more evidence or information becomes available.\n",
        "- In the context of the Naïve Bayes classifier, Bayes' theorem is used to calculate the probability of a certain class label given the observed features.\n",
        "\n",
        "\n",
        "      The formula for the Naïve Bayes classifier's classification decision is:\n",
        "          P(A|B) = P(B|A) * P(A) / P(B)\n",
        "\n",
        "- Where the probability that we are interested in calculating P(A|B) is called the posterior probability and the marginal probability of the event P(A) is called the prior.\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "a_CEAMDfupQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "---\n",
        "#### **What is the optimal Bayes classifier?**\n",
        "\n",
        "Sol:\n",
        "\n",
        "- The Optimal Bayes Classifier, also known as the Bayes Optimal Classifier or the Bayes Optimal Decision Rule, is a theoretical concept in the field of machine learning and statistics.\n",
        "\n",
        "- It represents an idealized approach to classification that aims to minimize the classification error by making decisions based on the true underlying probability distributions of the data.\n",
        "\n",
        "- The classifier is \"optimal\" in the sense that it achieves the lowest possible error rate among all possible classifiers, given the specific probability distributions of the data and the associated costs of utilities for different types of errors. (In other words, it provides the best achievable performance given the available information)\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "qS3DOVqaupT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **Write any two features of Bayesian learning methods.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**Probabilistic Framework:**\n",
        "- Bayesian learning methods are grounded in probability theory.\n",
        "- They treat model parameters as random variables and use probability distributions to represent uncertainty about these parameters.\n",
        "- This probabilistic approach allows for the incorporation of prior knowledge and updates beliefs about parameters based on observed data using Bayes' theorem.\n",
        "- This is particularly advantageous when dealing with limited or noisy data.\n",
        "\n",
        "**Regularization through Priors:**\n",
        "- Bayesian methods naturally incorporate regularization through the use of prior distributions.\n",
        "- By choosing appropriate priors, one can introduce constraints on the parameter space, leading to more stable and robust models.\n",
        "- The balance between the data likelihood and the prior information is determined by the observed data, which helps in avoiding overfitting or underfitting problems.\n",
        "- Regularization is automatically achieved through the posterior distribution, which takes into account both the data and the prior information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "DXavJu0supWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **7. Define the concept of consistent learners.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "- a consistent learner becomes increasingly accurate and approaches the true underlying relationship between the input features and the target output as more data is provided.\n",
        "\n",
        "- This property is essential because it indicates that, given sufficient data, the learner will make fewer and fewer mistakes and will eventually identify the optimal solution, assuming that the true relationship can be represented within the model class being considered.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "28v8Wy3DupZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **8. Write any two strengths of Bayes classifier.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**Strengths of Bayes Classifier:**\n",
        "\n",
        "- Theoretical Foundation: strong theoretical foundation ensures that the classifier's decisions are based on a solid understanding of the underlying data distributions.\n",
        "\n",
        "**Optimal Decision Rule:**\n",
        "- In cases where the assumptions of the Bayes classifier are met and the true probabilities are known, the Bayes classifier is the optimal decision rule that minimizes the misclassification rate.\n",
        "- It provides a benchmark against which other classifiers can be compared.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "CjJ1scHnupcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **9. Write any two weaknesses of Bayes classifier.**\n",
        "\n",
        "Sol:\n",
        "\n",
        "**Assumption of Independence:**\n",
        "- One of the main weaknesses of the Bayes classifier is its assumption of feature independence.\n",
        "\n",
        "- It assumes that the features are conditionally independent given the class label.\n",
        "\n",
        "- In many real-world scenarios, this assumption does not hold, leading to a decrease in classification\n",
        "accuracy.\n",
        "\n",
        "- This is particularly evident in cases where features are correlated.\n",
        "\n",
        "**Sensitivity to Feature Space Size:**\n",
        "- The Bayes classifier's performance can be impacted by the dimensionality of the feature space.\n",
        "\n",
        "- As the number of features increases, the required amount of training data to accurately estimate the class-conditional probabilities grows exponentially.\n",
        "\n",
        "- In high-dimensional spaces, obtaining enough data to reliably estimate these probabilities can be challenging, leading to overfitting or poor generalization.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "qvtz4FYxSLRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "#### **10. Explain how Naïve Bayes classifier is used for**\n",
        "**1. Text classification**\n",
        "\n",
        "Sol:\n",
        "\n",
        "- Text classification involves categorizing documents or pieces of text into predefined classes or categories. This can include tasks like sentiment analysis, topic classification, spam detection, and more. Naïve Bayes classifier is commonly used for text classification due to its effectiveness in handling large feature spaces and its ability to deal with categorical data (words or tokens in this case).\n",
        "\n",
        "- In text classification, the Naïve Bayes classifier estimates the likelihood of a particular class given the presence or absence of certain words (features) in the document. It assumes that the features (words) are conditionally independent given the class label, which simplifies the computation of the probabilities. The classifier calculates the probability of each class for a given document and assigns the document to the class with the highest probability.\n",
        "\n",
        "\n",
        "**2. Spam filtering**\n",
        "\n",
        "Sol:\n",
        "- Spam filtering involves classifying emails as either spam or legitimate (ham) based on their content. Naïve Bayes is a popular choice for this task because it can handle large volumes of text data efficiently and effectively. The classifier is trained on a labeled dataset of emails, where each email is associated with a class label (spam or ham).\n",
        "\n",
        "- To use Naïve Bayes for spam filtering, the classifier calculates the likelihood of a word appearing in spam and ham emails separately. When a new email arrives, the probabilities of the words in that email belonging to the spam and ham classes are calculated. These probabilities are combined using Bayes' theorem to determine the overall likelihood of the email being spam or ham.\n",
        "\n",
        "**3. Market sentiment analysis**\n",
        "Sol:\n",
        "\n",
        "- Market sentiment analysis involves determining the sentiment or emotional tone expressed in text data, often related to financial markets, products, or services. Naïve Bayes can be applied to classify text data as positive, negative, or neutral sentiment based on the words used in the text.\n",
        "\n",
        "- For market sentiment analysis, a Naïve Bayes classifier is trained on a labeled dataset of text samples associated with different sentiment labels. The classifier estimates the probabilities of different sentiment labels given the presence of specific words or tokens in the text. When new text data arrives, the classifier calculates the likelihood of each sentiment label and assigns the label with the highest likelihood as the predicted sentiment of the text.\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "bqoCG9LJSLVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "BGbmXjoxSLZN"
      }
    }
  ]
}